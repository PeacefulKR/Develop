{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b451a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c125ec84",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 263\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# 실행\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/asyncio/runners.py:190\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Chat LLM Orchestrator\n",
    "# ============================================\n",
    "\n",
    "class LLMProvider(Enum):\n",
    "    \"\"\"사용 가능한 LLM 제공자\"\"\"\n",
    "    OPENAI = \"openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    GOOGLE = \"google\"\n",
    "    COHERE = \"cohere\"\n",
    "\n",
    "@dataclass\n",
    "class LLMConfig:\n",
    "    \"\"\"LLM 설정\"\"\"\n",
    "    provider: LLMProvider\n",
    "    model: str\n",
    "    api_key: str\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 2048\n",
    "    timeout: int = 30\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    \"\"\"채팅 메시지\"\"\"\n",
    "    role: str  # \"user\", \"assistant\", \"system\"\n",
    "    content: str\n",
    "    timestamp: Optional[float] = None\n",
    "\n",
    "@dataclass\n",
    "class ChatResponse:\n",
    "    \"\"\"LLM 응답\"\"\"\n",
    "    content: str\n",
    "    provider: LLMProvider\n",
    "    model: str\n",
    "    usage: Dict[str, int]  # token usage\n",
    "    \n",
    "class ChatLLMOrchestrator:\n",
    "    \"\"\"여러 LLM을 조율하는 오케스트레이터\"\"\"\n",
    "    \n",
    "    def __init__(self, configs: List[LLMConfig], primary_provider: LLMProvider = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            configs: LLM 설정 리스트\n",
    "            primary_provider: 주요 LLM 제공자\n",
    "        \"\"\"\n",
    "        self.llm_configs: Dict[LLMProvider, LLMConfig] = {\n",
    "            config.provider: config for config in configs\n",
    "        }\n",
    "        self.primary_provider = primary_provider or configs[0].provider\n",
    "        self.conversation_history: List[Message] = []\n",
    "        self.response_cache: Dict[str, ChatResponse] = {}\n",
    "        \n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        \"\"\"대화 히스토리에 메시지 추가\"\"\"\n",
    "        import time\n",
    "        message = Message(role=role, content=content, timestamp=time.time())\n",
    "        self.conversation_history.append(message)\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"대화 히스토리 반환\"\"\"\n",
    "        return [\n",
    "            {\"role\": msg.role, \"content\": msg.content}\n",
    "            for msg in self.conversation_history\n",
    "        ]\n",
    "    \n",
    "    async def query_llm(\n",
    "        self,\n",
    "        provider: LLMProvider,\n",
    "        system_prompt: Optional[str] = None,\n",
    "        use_cache: bool = True\n",
    "    ) -> ChatResponse:\n",
    "        \"\"\"\n",
    "        특정 LLM에 쿼리 전송\n",
    "        \n",
    "        Args:\n",
    "            provider: 사용할 LLM 제공자\n",
    "            system_prompt: 시스템 프롬프트\n",
    "            use_cache: 캐시 사용 여부\n",
    "            \n",
    "        Returns:\n",
    "            ChatResponse: LLM 응답\n",
    "        \"\"\"\n",
    "        if provider not in self.llm_configs:\n",
    "            raise ValueError(f\"Unknown provider: {provider}\")\n",
    "        \n",
    "        # 캐시 확인\n",
    "        cache_key = self._generate_cache_key(provider, system_prompt)\n",
    "        if use_cache and cache_key in self.response_cache:\n",
    "            return self.response_cache[cache_key]\n",
    "        \n",
    "        config = self.llm_configs[provider]\n",
    "        \n",
    "        # 메시지 준비\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.extend(self.get_conversation_history())\n",
    "        \n",
    "        # 비동기로 LLM 호출 시뮬레이션\n",
    "        response = await self._call_llm_api(provider, config, messages)\n",
    "        \n",
    "        # 캐시에 저장\n",
    "        if use_cache:\n",
    "            self.response_cache[cache_key] = response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    async def query_primary(\n",
    "        self,\n",
    "        system_prompt: Optional[str] = None\n",
    "    ) -> ChatResponse:\n",
    "        \"\"\"주요 LLM에 쿼리\"\"\"\n",
    "        return await self.query_llm(self.primary_provider, system_prompt)\n",
    "    \n",
    "    async def query_all(\n",
    "        self,\n",
    "        system_prompt: Optional[str] = None\n",
    "    ) -> Dict[LLMProvider, ChatResponse]:\n",
    "        \"\"\"모든 LLM에 쿼리\"\"\"\n",
    "        tasks = [\n",
    "            self.query_llm(provider, system_prompt)\n",
    "            for provider in self.llm_configs.keys()\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return {\n",
    "            provider: result\n",
    "            for provider, result in zip(self.llm_configs.keys(), results)\n",
    "        }\n",
    "    \n",
    "    async def compare_responses(\n",
    "        self,\n",
    "        system_prompt: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"모든 LLM의 응답 비교\"\"\"\n",
    "        responses = await self.query_all(system_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"responses\": {\n",
    "                provider.value: response.content\n",
    "                for provider, response in responses.items()\n",
    "            },\n",
    "            \"usage\": {\n",
    "                provider.value: response.usage\n",
    "                for provider, response in responses.items()\n",
    "            },\n",
    "            \"comparison\": {\n",
    "                \"total_providers\": len(responses),\n",
    "                \"primary_response\": responses[self.primary_provider].content\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def clear_history(self) -> None:\n",
    "        \"\"\"대화 히스토리 초기화\"\"\"\n",
    "        self.conversation_history.clear()\n",
    "    \n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"캐시 초기화\"\"\"\n",
    "        self.response_cache.clear()\n",
    "    \n",
    "    def _generate_cache_key(self, provider: LLMProvider, system_prompt: Optional[str]) -> str:\n",
    "        \"\"\"캐시 키 생성\"\"\"\n",
    "        history_str = json.dumps(self.get_conversation_history())\n",
    "        prompt_str = system_prompt or \"\"\n",
    "        key_data = f\"{provider.value}_{prompt_str}_{history_str}\"\n",
    "        import hashlib\n",
    "        return hashlib.md5(key_data.encode()).hexdigest()\n",
    "    \n",
    "    async def _call_llm_api(\n",
    "        self,\n",
    "        provider: LLMProvider,\n",
    "        config: LLMConfig,\n",
    "        messages: List[Dict[str, str]]\n",
    "    ) -> ChatResponse:\n",
    "        \"\"\"\n",
    "        LLM API 호출 (구현 예시)\n",
    "        \n",
    "        실제 구현에서는 각 제공자의 SDK 사용\n",
    "        \"\"\"\n",
    "        # 시뮬레이션: 실제로는 API 호출\n",
    "        await asyncio.sleep(0.5)  # API 호출 시뮬레이션\n",
    "        \n",
    "        response_content = f\"Response from {provider.value} ({config.model})\"\n",
    "        \n",
    "        return ChatResponse(\n",
    "            content=response_content,\n",
    "            provider=provider,\n",
    "            model=config.model,\n",
    "            usage={\n",
    "                \"prompt_tokens\": 10,\n",
    "                \"completion_tokens\": 20,\n",
    "                \"total_tokens\": 30\n",
    "            }\n",
    "        )\n",
    "\n",
    "# ============================================\n",
    "# 사용 예시\n",
    "# ============================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Chat LLM Orchestrator 사용 예시\"\"\"\n",
    "    \n",
    "    # LLM 설정\n",
    "    configs = [\n",
    "        LLMConfig(\n",
    "            provider=LLMProvider.OPENAI,\n",
    "            model=\"gpt-4\",\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\", \"dummy-key\"),\n",
    "            temperature=0.7\n",
    "        ),\n",
    "        LLMConfig(\n",
    "            provider=LLMProvider.ANTHROPIC,\n",
    "            model=\"claude-3\",\n",
    "            api_key=os.getenv(\"ANTHROPIC_API_KEY\", \"dummy-key\"),\n",
    "            temperature=0.5\n",
    "        ),\n",
    "        LLMConfig(\n",
    "            provider=LLMProvider.GOOGLE,\n",
    "            model=\"gemini-pro\",\n",
    "            api_key=os.getenv(\"GOOGLE_API_KEY\", \"dummy-key\"),\n",
    "            temperature=0.8\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # 오케스트레이터 생성\n",
    "    orchestrator = ChatLLMOrchestrator(\n",
    "        configs=configs,\n",
    "        primary_provider=LLMProvider.OPENAI\n",
    "    )\n",
    "    \n",
    "    # 대화 추가\n",
    "    orchestrator.add_message(\"user\", \"안녕하세요, LLM 오케스트레이터입니다.\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Chat LLM Orchestrator 시작\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. 주요 LLM에만 쿼리\n",
    "    print(\"\\n[1] 주요 LLM(OpenAI) 응답:\")\n",
    "    response = await orchestrator.query_primary(\n",
    "        system_prompt=\"당신은 유용한 어시스턴트입니다.\"\n",
    "    )\n",
    "    print(f\"Provider: {response.provider.value}\")\n",
    "    print(f\"Content: {response.content}\")\n",
    "    print(f\"Usage: {response.usage}\")\n",
    "    \n",
    "    # 2. 모든 LLM 비교\n",
    "    print(\"\\n[2] 모든 LLM 응답 비교:\")\n",
    "    comparison = await orchestrator.compare_responses(\n",
    "        system_prompt=\"당신은 유용한 어시스턴트입니다.\"\n",
    "    )\n",
    "    print(json.dumps(comparison, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # 3. 대화 히스토리 확인\n",
    "    print(\"\\n[3] 대화 히스토리:\")\n",
    "    for i, msg in enumerate(orchestrator.conversation_history, 1):\n",
    "        print(f\"  {i}. [{msg.role}] {msg.content}\")\n",
    "    \n",
    "    # 4. 캐시 상태\n",
    "    print(f\"\\n[4] 캐시 크기: {len(orchestrator.response_cache)} items\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d1c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
